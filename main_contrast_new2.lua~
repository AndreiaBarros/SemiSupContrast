require 'xlua'
require 'optim'
require 'pl'
require 'trepl'
require 'nn'
require 'data'
require 'utils.log'
require 'utils.plotCSV'
-- require 'CMECriterion'
require 'ContrastingMinEntropyCriterion'
local tnt = require 'torchnet'
----------------------------------------------------------------------
local cmd = torch.CmdLine()
cmd:addTime()
cmd:text()
cmd:text('Training a convolutional network for visual classification')
cmd:text()
cmd:text('==>Options')

cmd:text('===>Model And Training Regime')
cmd:option('-modelsFolder', './models/', 'Models Folder')
cmd:option('-model', 'MNIST_contrast.lua', 'Model file - must return valid network.')
cmd:option('-LR', 0.1, 'learning rate')
cmd:option('-LRDecay', 0, 'learning rate decay (in # samples)')
cmd:option('-weightDecay', 1e-4, 'L2 penalty on the weights')
cmd:option('-momentum', 0.9, 'momentum')
cmd:option('-batchSize', 128, 'batch size')
cmd:option('-optimization', 'sgd', 'optimization method')
cmd:option('-epoch', -1, 'number of epochs to train, -1 for unbounded')
cmd:option('-evalN', 100000, 'evaluate every N samples')
cmd:option('-topK', 1, 'measure top k error')
cmd:option('-crossContrast', true, '')
cmd:option('-numLabeled', 10, '')
cmd:option('-numCompared', 1, '')
cmd:option('-testOnly', false, '')
cmd:option('-saveFeats', true, '')

cmd:text('===>Platform Optimization')
cmd:option('-threads', 8, 'number of threads')
cmd:option('-seed', 1, 'seed number')
cmd:option('-type', 'cuda', 'cuda/cl/float/double')
cmd:option('-devid', 1, 'device ID (if using CUDA)')
cmd:option('-saveOptState', false, 'Save optimization state every epoch')

cmd:text('===>Save/Load Options')
cmd:option('-load', '', 'load existing net')
cmd:option('-resume', false, 'resume training from the same epoch')
cmd:option('-save', os.date():gsub(' ',''), 'name of saved directory')

cmd:text('===>Data Options')
cmd:option('-dataset', 'MNIST', 'Dataset - ImageNet, Cifar10, Cifar100, STL10, SVHN, MNIST')
cmd:option('-augment', false, 'Augment training data')

opt = cmd:parse(arg or {})
opt.model = opt.modelsFolder .. paths.basename(opt.model, '.lua')
opt.savePath = paths.concat('./results', opt.save)
torch.setnumthreads(opt.threads)
torch.manualSeed(opt.seed)
torch.setdefaulttensortype('torch.FloatTensor')

-- Output files configuration
os.execute('mkdir -p ' .. opt.savePath)
cmd:log(opt.savePath .. '/Log.txt', opt)
local logFile = paths.concat(opt.savePath, 'LogTable.csv')
local classTopK = table.prune({1, opt.topK})

local log = getLog{
  logFile = logFile,
  keys = {
    [1] = 'Epoch', [2] = 'Train Loss', [3] = 'Test Loss',
    ['Train Error'] = classTopK, ['Test Error'] = classTopK
  },
  nestFormat ='%s (Top %s)'
}

local plots = {
  {
    title = opt.save .. ':Loss',
    labels = {'Epoch', 'Train Loss', 'Test Loss'},
    ylabel = 'Loss'
  }
}

for i,k in pairs(classTopK) do
  table.insert(plots,
    {
      title = ('%s : Classification Error (Top %s)'):format(opt.save, k),
      labels = {'Epoch', ('Train Error (Top %s)'):format(k), ('Test Error (Top %s)'):format(k)},
      ylabel = 'Error %'
    }
  )
end

log:attach('onFlush',
  {
    function()
      local plot = PlotCSV(logFile)
      plot:parse()
      for _,p in pairs(plots) do
        if pcall(require , 'display') then
          p.win = plot:display(p)
        end
        plot:save(paths.concat(opt.savePath, p.title:gsub('%s','') .. '.eps'), p)
      end
    end
  }
)

local netFilename = paths.concat(opt.savePath, 'savedModel')

----------------------------------------------------------------------
local config = {
  inputSize = 32,
  reshapeSize = 32,
  inputMean = 128,
  inputStd = 128,
  regime = {}
}
local function setConfig(target, origin, overwrite)
  for key in pairs(config) do
    if overwrite or target[key] == nil then
      target[key] = origin[key]
    end
  end
end

-- Model + Loss:
local model, criterion
if paths.filep(opt.load) then
  local conf, criterion = require(opt.model)
  model = torch.load(opt.load)
  if not opt.resume then
    setConfig(model, conf)
  end
else
  model, criterion = require(opt.model)
end

criterion = nn.ContrastingMinEntropyCriterion(2)

setConfig(model, config)

-- Data preparation
local trainData = getDataset(opt.dataset, 'train')
local testData = getDataset(opt.dataset, 'test')
-- classes
local classes = trainData:classes()
local numClasses = #classes

local evalTransform = tnt.transform.compose{
  Scale(model.reshapeSize),
  CenterCrop(model.inputSize),
  Normalize(model.inputMean, model.inputStd)
}

local augTransform = tnt.transform.compose{
  --RandomScale(model.inputSize, model.reshapeSize * 1.2),
  -- Scale(model.reshapeSize),
  -- CenterCrop(model.inputSize),
  RandomCrop(model.inputSize, 4),
  HorizontalFlip(),
  Normalize(model.inputMean,model.inputStd),
  -- ColorJitter(0.4,0.4,0.4)
}

testData = tnt.TransformDataset{
  transforms = {
    input = evalTransform
  },
  dataset = testData
}

trainData = tnt.TransformDataset{
  transforms = {
    input = (opt.augment and augTransform) or evalTransform
  },
  dataset = trainData:shuffle()
}

local labeledData, unlabeledData = extractEachTarget{dataset=trainData,num=opt.numLabeled}

----------------------------------------------------------------------
-- Model optimization

local types = {
  cuda = 'torch.CudaTensor',
  float = 'torch.FloatTensor',
  cl = 'torch.ClTensor',
  double = 'torch.DoubleTensor'
}

local tensorType = types[opt.type] or 'torch.FloatTensor'

if opt.type == 'cuda' then
  require 'cutorch'
  require 'cunn'
  cutorch.setDevice(opt.devid)
  cutorch.manualSeed(opt.seed)
  local cudnnAvailable = pcall(require , 'cudnn')
  if cudnnAvailable then
    cudnn.benchmark = true
    model = cudnn.convert(model, cudnn)
  end
elseif opt.type == 'cl' then
  require 'cltorch'
  require 'clnn'
  cltorch.setDevice(opt.devid)
end

model:type(tensorType)
criterion = criterion:type(tensorType)

-- Optimization configuration
local net = model
local clone1 = net:clone('weight', 'bias', 'gradWeight', 'gradBias', 'running_mean', 'running_std', 'running_var')
local clone2 = net:clone('weight', 'bias', 'gradWeight', 'gradBias', 'running_mean', 'running_std', 'running_var')
model = nn.ParallelTable():add(net):add(clone1):add(clone2)

setConfig(model, net, true)

local weights,gradients = model:getParameters()
local savedModel = model

savedModel = savedModel:clone('weight', 'bias', 'running_mean', 'running_std', 'running_var')

------------------Optimization Configuration--------------------------
local optimState = model.optimState or {
  method = opt.optimization,
  learningRate = opt.LR,
  momentum = opt.momentum,
  dampening = 0,
  weightDecay = opt.weightDecay,
  learningRateDecay = opt.LRDecay
}

----------------------------------------------------------------------
print '==> Network'
print(model)
print('==>' .. weights:nElement() .. ' Parameters')

print '==> Criterion'
print(criterion)

----------------------------------------------------------------------
if opt.savePathOptState then
  model.optimState = optimState
end

local epoch = (model.epoch and model.epoch + 1) or 1

local function forward(labeledData, unlabeledData, train)

  local trainLabeledData = tnt.ConcatDataset{datasets=labeledData}
  local sizeData = math.max(unlabeledData:size(), trainLabeledData:size())

  local unlabeledBatch = unlabeledData:shuffle(sizeData, true):batch(opt.batchSize)
  local labeledBatch = trainLabeledData:shuffle(sizeData, true):batch(opt.batchSize)
  local byClass = {}
  for i,d in pairs(labeledData) do
    byClass[i] = d:shuffle(sizeData*opt.numCompared, true)
  end
  local contrast = tnt.MergeDataset{datasets=byClass}:batch(opt.numCompared)

  local labeledIter = getIterator(labeledBatch, opt.threads, opt.seed)()
  local unlabeledIter = getIterator(unlabeledBatch, opt.threads, opt.seed)()
  local contrastIter = getIterator(contrast, opt.threads, opt.seed)()
  local numBatches = labeledBatch:size()
  local xL = torch.Tensor():type(tensorType)
  local xU = torch.Tensor():type(tensorType)
  local xC = torch.Tensor():type(tensorType)
  local target = torch.Tensor():type(tensorType)
  local numSamples = 0
  local lossMeter = tnt.AverageValueMeter()
  local classMeter = tnt.ClassErrorMeter{topk = classTopK}
  lossMeter:reset(); classMeter:reset()

  local function copyBatch(x,b, field)
    local field = field or 'input'
    x:resize(b[field]:size()):copy(b[field])
  end

  for i=1, numBatches do
    t = torch.tic()
    copyBatch(xU, unlabeledIter())
    copyBatch(xC, contrastIter())
    local labeled = labeledIter()
    copyBatch(xL, labeled)
    copyBatch(target, labeled,'target')

    local x = {xL, xU, xC:select(1,1)}
    local y = model:forward(x)

    local yL, yU, yC = unpack(y)
    local loss = criterion:forward(y, target:squeeze())

    if train then
      local function feval()
        model:zeroGradParameters()
        local dE_dy = criterion:backward(y, target:squeeze())
        model:backward(x, dE_dy)
        return loss, gradients
      end
      _G.optim[optimState.method](feval, weights, optimState)

    end
    -- classMeter:add(assignment, target:squeeze())
    lossMeter:add(loss, xL:size(1) / opt.batchSize)
    numSamples = numSamples + xL:size(1)
    xlua.progress(numSamples, sizeData)
    if numSamples % opt.evalN < opt.batchSize then
      print('Current Loss: ' .. lossMeter:value())
      print('Current Error: ' .. classMeter:value()[1])
    end
  end
  return lossMeter:value(), classMeter:value()
end

------------------------------
local function train(labeledData, unlabeledData)
  model:training()
  return forward(labeledData, unlabeledData, true)
end

local function test(labeledData, unlabeledData)
  model:evaluate()
  return forward(labeledData, unlabeledData, false)
end
------------------------------

local function extractFeats(model, data)
  local sizeData = data:size()
  local Feats = torch.Tensor():type(tensorType)
  local Labels = torch.Tensor():type(tensorType)
  local first = true
  local x = torch.Tensor():type(tensorType)
  local num = 1
  local batch = data:batch(opt.batchSize)
  for i=1, batch:size() do
    local sample = batch:get(i)
    x:resize(sample.input:size()):copy(sample.input)
    local y = model:forward(x)
    if first then
      Feats:resize(sizeData, y:size(2))
      Labels:resize(sizeData)
    end
    Feats:narrow(1,num,x:size(1)):copy(y)
    Labels:narrow(1,num,x:size(1)):copy(sample.target)
    num = num + x:size(1)
  end
  return Feats, Labels
end

-- function that performs nearest neighbor classification:
local function nn_classification(train_Z, train_Y, test_Z)

  -- compute squared Euclidean distance matrix between train and test data:
  local N = train_Z:size(1)
  local M = test_Z:size(1)
  local buff1 = train_Z.new():resize(train_Z:size())
  local buff2 = test_Z.new():resize(test_Z:size())
  torch.cmul(buff1, train_Z, train_Z)
  torch.cmul(buff2, test_Z, test_Z)
  local sum_Z1 = buff1:sum(2)
  local sum_Z2 = buff2:sum(2)
  local sum_Z1_expand = sum_Z1:t():expand(M, N)
  local sum_Z2_expand = sum_Z2:expand(M, N)
  local D = torch.mm(test_Z, train_Z:t())
  D:mul(-2)
  D:add(sum_Z1_expand):add(sum_Z2_expand)

  -- perform 1-nearest neighbor classification:
  local test_Y = train_Y.new():resize(M)
  for m = 1, M do
    local _,ind = torch.min(D[m], 1)
    test_Y[m] = train_Y[ind[1]]
  end

  -- return classification
  return test_Y
end

local function test_nn(model, labeledData, testData)
  model:evaluate()
  local trainLabeledData = tnt.ConcatDataset{datasets=labeledData}

  local trainFeats, trainLabels = extractFeats(model, trainLabeledData, trainLabeledData:size())
  local testFeats, testLabels = extractFeats(model, testData, testData:size())
  if opt.saveFeats then
    torch.save(paths.concat(opt.savePath, 'trainFeats_epoch' .. epoch), {trainFeats:float(), trainLabels:float()})
    torch.save(paths.concat(opt.savePath, 'testFeats_epoch' .. epoch), {testFeats:float(), testLabels:float()})
  end
  testPred = nn_classification(trainFeats, trainLabels, testFeats)
  return testPred:ne(testLabels):float():mean() * 100
end

local lowestTestError = 100
print '\n==> Starting Training\n'

while epoch ~= opt.epoch do

  if not opt.testOnly then
    model.epoch = epoch
    log:set{Epoch = epoch}
    print('\nEpoch ' .. epoch)
    updateOpt(optimState, epoch, model.regime, true)
    print('Training:')
    --Train
    local trainLoss, trainClassError = train(labeledData, unlabeledData)
    trainClassError[1] = 0
    log:set{['Train Loss'] = trainLoss, ['Train Error'] = trainClassError}
    -- torch.save(netFilename, savedModel)
  end
  local nnError = test_nn(net, labeledData, testData)
  print('Nearest Neighbor Error: ' .. nnError)

  --Test
  print('Test:')
  local testLoss,testClassError = test(labeledData, testData)
  print(testClassError)
  testClassError[1] = nnError
  log:set{['Test Loss'] = testLoss, ['Test Error'] = testClassError}

  if opt.testOnly then
    break
  end
  log:flush()

  if lowestTestError > testClassError[1] then
    lowestTestError = testClassError[1]
    os.execute(('cp %s %s'):format(netFilename, netFilename .. '_best'))
  end

  epoch = epoch + 1
end
